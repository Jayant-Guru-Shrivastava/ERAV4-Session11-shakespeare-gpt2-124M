{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tPt_aWn4mxn",
        "outputId": "58498344-1104-4fdb-bf24-3af7d2e18093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ERAV4-Session11-shakespeare-gpt2-124M'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 9 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (9/9), 429.47 KiB | 18.67 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Jayant-Guru-Shrivastava/ERAV4-Session11-shakespeare-gpt2-124M.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ERAV4-Session11-shakespeare-gpt2-124M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD1KbepQ45FR",
        "outputId": "96d7086e-8f77-413c-ec93-e4d0dfa45bf7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ERAV4-Session11-shakespeare-gpt2-124M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFo9__nS5MKZ",
        "outputId": "4252bf75-e796-4836-ec7c-c0c3909e8de2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.8.0+cu126)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (5.49.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.121.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.36.0)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.14.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 3)) (0.38.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio->-r requirements.txt (line 3)) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 3)) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio->-r requirements.txt (line 3)) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 3)) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 3)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 3)) (0.16.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 3)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 3)) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 3)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 3)) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 3)) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 3)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPy0NQf75YJi",
        "outputId": "3725dd3b-2524-4b41-8b03-6a44165895e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tdata.py  input.txt  model.py  README.md  requirements.txt  train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsUMYxnO6Im0",
        "outputId": "804f348c-828b-4629-937c-f07dc7c2ec7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loaded 338025 tokens from input.txt\n",
            "With B=8, T=256, we have 165 batches per epoch\n",
            "Number of parameters: 123.849984 M\n",
            "step     10 | loss 8.460716 | best 8.460716 | 5.0s\n",
            "step     20 | loss 7.688958 | best 7.688958 | 9.8s\n",
            "step     30 | loss 7.206179 | best 7.206179 | 14.7s\n",
            "step     40 | loss 6.972393 | best 6.946067 | 19.6s\n",
            "step     50 | loss 6.921648 | best 6.778133 | 24.5s\n",
            "step     60 | loss 6.577611 | best 6.537043 | 29.4s\n",
            "step     70 | loss 6.385334 | best 6.350562 | 34.4s\n",
            "step     80 | loss 6.135435 | best 6.125066 | 39.3s\n",
            "step     90 | loss 6.411036 | best 6.125066 | 44.3s\n",
            "step    100 | loss 6.021526 | best 5.908682 | 49.2s\n",
            "step    110 | loss 6.132651 | best 5.908682 | 54.1s\n",
            "step    120 | loss 6.175994 | best 5.908682 | 59.0s\n",
            "step    130 | loss 6.101154 | best 5.744495 | 63.9s\n",
            "step    140 | loss 5.920246 | best 5.744495 | 68.8s\n",
            "step    150 | loss 6.107786 | best 5.744495 | 73.7s\n",
            "step    160 | loss 5.943058 | best 5.452579 | 78.5s\n",
            "step    170 | loss 6.003754 | best 5.452579 | 83.4s\n",
            "step    180 | loss 5.606263 | best 5.373715 | 88.2s\n",
            "step    190 | loss 6.064153 | best 5.373715 | 93.1s\n",
            "step    200 | loss 5.612759 | best 5.373715 | 97.9s\n",
            "step    210 | loss 5.958136 | best 5.373715 | 102.8s\n",
            "step    220 | loss 5.814340 | best 5.373715 | 107.7s\n",
            "step    230 | loss 5.648401 | best 5.366078 | 112.5s\n",
            "step    240 | loss 5.685418 | best 5.366078 | 117.4s\n",
            "step    250 | loss 5.672336 | best 5.366078 | 122.3s\n",
            "step    260 | loss 5.196486 | best 5.196486 | 127.2s\n",
            "step    270 | loss 5.946486 | best 5.196486 | 132.1s\n",
            "step    280 | loss 5.885275 | best 5.196486 | 137.0s\n",
            "step    290 | loss 5.624204 | best 5.196486 | 141.9s\n",
            "step    300 | loss 5.733867 | best 5.196486 | 146.8s\n",
            "step    310 | loss 5.608083 | best 5.152324 | 151.7s\n",
            "step    320 | loss 5.085544 | best 5.085544 | 156.6s\n",
            "step    330 | loss 5.485435 | best 4.724350 | 161.5s\n",
            "step    340 | loss 5.327749 | best 4.724350 | 166.4s\n",
            "step    350 | loss 5.098165 | best 4.724350 | 171.3s\n",
            "step    360 | loss 5.191334 | best 4.724350 | 176.2s\n",
            "step    370 | loss 5.190502 | best 4.724350 | 181.1s\n",
            "step    380 | loss 5.675803 | best 4.724350 | 186.0s\n",
            "step    390 | loss 5.350692 | best 4.724350 | 190.8s\n",
            "step    400 | loss 5.316509 | best 4.724350 | 195.7s\n",
            "step    410 | loss 5.081687 | best 4.724350 | 200.6s\n",
            "step    420 | loss 5.504212 | best 4.724350 | 205.5s\n",
            "step    430 | loss 5.090055 | best 4.724350 | 210.4s\n",
            "step    440 | loss 5.180219 | best 4.724350 | 215.3s\n",
            "step    450 | loss 5.348443 | best 4.724350 | 220.2s\n",
            "step    460 | loss 5.139428 | best 4.724350 | 225.1s\n",
            "step    470 | loss 4.871051 | best 4.724350 | 230.0s\n",
            "step    480 | loss 5.161832 | best 4.724350 | 234.9s\n",
            "step    490 | loss 5.096089 | best 4.298034 | 239.8s\n",
            "step    500 | loss 5.291711 | best 4.298034 | 244.7s\n",
            "step    510 | loss 4.842820 | best 4.298034 | 249.6s\n",
            "step    520 | loss 5.250027 | best 4.298034 | 254.5s\n",
            "step    530 | loss 4.866520 | best 4.298034 | 259.4s\n",
            "step    540 | loss 5.269000 | best 4.298034 | 264.3s\n",
            "step    550 | loss 5.100640 | best 4.298034 | 269.2s\n",
            "step    560 | loss 4.962352 | best 4.298034 | 274.1s\n",
            "step    570 | loss 5.044948 | best 4.298034 | 279.0s\n",
            "step    580 | loss 5.119598 | best 4.298034 | 283.9s\n",
            "step    590 | loss 4.491436 | best 4.298034 | 288.8s\n",
            "step    600 | loss 5.329103 | best 4.298034 | 293.7s\n",
            "step    610 | loss 5.301072 | best 4.298034 | 298.6s\n",
            "step    620 | loss 4.985168 | best 4.298034 | 303.5s\n",
            "step    630 | loss 5.045115 | best 4.298034 | 308.4s\n",
            "step    640 | loss 4.988950 | best 4.298034 | 313.3s\n",
            "step    650 | loss 4.448606 | best 4.298034 | 318.2s\n",
            "step    660 | loss 4.927216 | best 4.001544 | 323.1s\n",
            "step    670 | loss 4.789720 | best 4.001544 | 328.0s\n",
            "step    680 | loss 4.672257 | best 4.001544 | 332.9s\n",
            "step    690 | loss 4.690042 | best 4.001544 | 337.8s\n",
            "step    700 | loss 4.442202 | best 4.001544 | 342.7s\n",
            "step    710 | loss 5.146673 | best 4.001544 | 347.6s\n",
            "step    720 | loss 4.857786 | best 4.001544 | 352.5s\n",
            "step    730 | loss 4.882207 | best 4.001544 | 357.4s\n",
            "step    740 | loss 4.610674 | best 4.001544 | 362.3s\n",
            "step    750 | loss 5.088376 | best 4.001544 | 367.1s\n",
            "step    760 | loss 4.611421 | best 4.001544 | 372.0s\n",
            "step    770 | loss 4.714744 | best 4.001544 | 376.9s\n",
            "step    780 | loss 4.920251 | best 4.001544 | 381.8s\n",
            "step    790 | loss 4.675858 | best 4.001544 | 386.7s\n",
            "step    800 | loss 4.423817 | best 4.001544 | 391.6s\n",
            "step    810 | loss 4.659202 | best 4.001544 | 396.5s\n",
            "step    820 | loss 4.733236 | best 3.774227 | 401.4s\n",
            "step    830 | loss 4.883237 | best 3.774227 | 406.3s\n",
            "step    840 | loss 4.468748 | best 3.774227 | 411.2s\n",
            "step    850 | loss 4.754346 | best 3.774227 | 416.2s\n",
            "step    860 | loss 4.420126 | best 3.774227 | 421.1s\n",
            "step    870 | loss 4.851269 | best 3.774227 | 426.0s\n",
            "step    880 | loss 4.666625 | best 3.774227 | 430.9s\n",
            "step    890 | loss 4.613926 | best 3.774227 | 435.8s\n",
            "step    900 | loss 4.604746 | best 3.774227 | 440.7s\n",
            "step    910 | loss 4.727443 | best 3.774227 | 445.6s\n",
            "step    920 | loss 3.989161 | best 3.774227 | 450.5s\n",
            "step    930 | loss 4.989339 | best 3.774227 | 455.4s\n",
            "step    940 | loss 4.890693 | best 3.774227 | 460.3s\n",
            "step    950 | loss 4.532707 | best 3.774227 | 465.2s\n",
            "step    960 | loss 4.617720 | best 3.774227 | 470.1s\n",
            "step    970 | loss 4.637853 | best 3.774227 | 475.0s\n",
            "step    980 | loss 4.060994 | best 3.774227 | 479.9s\n",
            "step    990 | loss 4.491815 | best 3.595721 | 484.8s\n",
            "step   1000 | loss 4.407126 | best 3.595721 | 489.7s\n",
            "step   1010 | loss 4.410388 | best 3.595721 | 494.6s\n",
            "step   1020 | loss 4.296389 | best 3.595721 | 499.5s\n",
            "step   1030 | loss 4.089920 | best 3.595721 | 504.4s\n",
            "step   1040 | loss 4.794206 | best 3.595721 | 509.3s\n",
            "step   1050 | loss 4.530626 | best 3.595721 | 514.2s\n",
            "step   1060 | loss 4.538453 | best 3.595721 | 519.1s\n",
            "step   1070 | loss 4.279430 | best 3.595721 | 524.0s\n",
            "step   1080 | loss 4.817336 | best 3.595721 | 528.9s\n",
            "step   1090 | loss 4.314162 | best 3.595721 | 533.8s\n",
            "step   1100 | loss 4.400575 | best 3.595721 | 538.7s\n",
            "step   1110 | loss 4.612627 | best 3.595721 | 543.6s\n",
            "step   1120 | loss 4.433190 | best 3.595721 | 548.5s\n",
            "step   1130 | loss 4.170504 | best 3.595721 | 553.4s\n",
            "step   1140 | loss 4.326622 | best 3.595721 | 558.3s\n",
            "step   1150 | loss 4.483425 | best 3.472916 | 563.2s\n",
            "step   1160 | loss 4.561029 | best 3.472916 | 568.1s\n",
            "step   1170 | loss 4.172802 | best 3.472916 | 572.9s\n",
            "step   1180 | loss 4.374634 | best 3.472916 | 577.8s\n",
            "step   1190 | loss 4.092512 | best 3.472916 | 582.7s\n",
            "step   1200 | loss 4.567482 | best 3.472916 | 587.6s\n",
            "step   1210 | loss 4.385414 | best 3.472916 | 592.5s\n",
            "step   1220 | loss 4.289707 | best 3.472916 | 597.4s\n",
            "step   1230 | loss 4.262373 | best 3.472916 | 602.3s\n",
            "step   1240 | loss 4.435497 | best 3.472916 | 607.2s\n",
            "step   1250 | loss 3.683010 | best 3.472916 | 612.1s\n",
            "step   1260 | loss 4.677475 | best 3.472916 | 617.0s\n",
            "step   1270 | loss 4.596552 | best 3.472916 | 621.9s\n",
            "step   1280 | loss 4.202632 | best 3.472916 | 626.8s\n",
            "step   1290 | loss 4.331623 | best 3.472916 | 631.7s\n",
            "step   1300 | loss 4.330168 | best 3.472916 | 636.6s\n",
            "step   1310 | loss 3.859037 | best 3.472916 | 641.5s\n",
            "step   1320 | loss 4.148705 | best 3.358931 | 646.4s\n",
            "step   1330 | loss 4.126608 | best 3.358931 | 651.3s\n",
            "step   1340 | loss 4.140278 | best 3.358931 | 656.2s\n",
            "step   1350 | loss 3.971179 | best 3.358931 | 661.1s\n",
            "step   1360 | loss 3.743143 | best 3.358931 | 666.0s\n",
            "step   1370 | loss 4.452321 | best 3.358931 | 670.9s\n",
            "step   1380 | loss 4.256316 | best 3.358931 | 675.8s\n",
            "step   1390 | loss 4.257319 | best 3.358931 | 680.7s\n",
            "step   1400 | loss 4.022607 | best 3.358931 | 685.6s\n",
            "step   1410 | loss 4.527245 | best 3.358931 | 690.5s\n",
            "step   1420 | loss 4.039163 | best 3.358931 | 695.4s\n",
            "step   1430 | loss 4.140233 | best 3.358931 | 700.3s\n",
            "step   1440 | loss 4.292603 | best 3.358931 | 705.2s\n",
            "step   1450 | loss 4.174395 | best 3.358931 | 710.1s\n",
            "step   1460 | loss 3.878564 | best 3.358931 | 715.0s\n",
            "step   1470 | loss 4.056664 | best 3.358931 | 719.9s\n",
            "step   1480 | loss 4.184176 | best 3.235986 | 724.8s\n",
            "step   1490 | loss 4.244397 | best 3.235986 | 729.7s\n",
            "step   1500 | loss 3.923410 | best 3.235986 | 734.6s\n",
            "step   1510 | loss 4.015647 | best 3.235986 | 739.5s\n",
            "step   1520 | loss 3.777812 | best 3.235986 | 744.4s\n",
            "step   1530 | loss 4.270674 | best 3.235986 | 749.3s\n",
            "step   1540 | loss 4.106933 | best 3.235986 | 754.2s\n",
            "step   1550 | loss 3.938759 | best 3.235986 | 759.1s\n",
            "step   1560 | loss 3.937544 | best 3.235986 | 764.0s\n",
            "step   1570 | loss 4.099533 | best 3.235986 | 768.9s\n",
            "step   1580 | loss 3.420866 | best 3.235986 | 773.8s\n",
            "step   1590 | loss 4.289095 | best 3.235986 | 778.8s\n",
            "step   1600 | loss 4.268602 | best 3.235986 | 783.7s\n",
            "step   1610 | loss 3.815018 | best 3.235986 | 788.6s\n",
            "step   1620 | loss 4.027425 | best 3.235986 | 793.5s\n",
            "step   1630 | loss 4.006524 | best 3.235986 | 798.4s\n",
            "step   1640 | loss 3.571960 | best 3.235986 | 803.3s\n",
            "step   1650 | loss 3.846407 | best 3.186025 | 808.2s\n",
            "step   1660 | loss 3.875803 | best 3.186025 | 813.1s\n",
            "step   1670 | loss 3.894963 | best 3.186025 | 818.0s\n",
            "step   1680 | loss 3.613193 | best 3.186025 | 822.9s\n",
            "step   1690 | loss 3.477693 | best 3.186025 | 827.8s\n",
            "step   1700 | loss 4.149417 | best 3.186025 | 832.7s\n",
            "step   1710 | loss 4.006567 | best 3.186025 | 837.6s\n",
            "step   1720 | loss 3.922664 | best 3.186025 | 842.5s\n",
            "step   1730 | loss 3.711551 | best 3.186025 | 847.4s\n",
            "step   1740 | loss 4.194095 | best 3.186025 | 852.3s\n",
            "step   1750 | loss 3.739540 | best 3.186025 | 857.2s\n",
            "step   1760 | loss 3.802256 | best 3.186025 | 862.1s\n",
            "step   1770 | loss 3.985965 | best 3.186025 | 867.0s\n",
            "step   1780 | loss 3.925384 | best 3.186025 | 871.9s\n",
            "step   1790 | loss 3.634909 | best 3.186025 | 876.8s\n",
            "step   1800 | loss 3.808585 | best 3.186025 | 881.7s\n",
            "step   1810 | loss 3.935065 | best 3.035654 | 886.6s\n",
            "step   1820 | loss 4.002209 | best 3.035654 | 891.5s\n",
            "step   1830 | loss 3.674011 | best 3.035654 | 896.4s\n",
            "step   1840 | loss 3.682757 | best 3.035654 | 901.3s\n",
            "step   1850 | loss 3.539592 | best 3.035654 | 906.2s\n",
            "step   1860 | loss 3.979995 | best 3.035654 | 911.1s\n",
            "step   1870 | loss 3.770132 | best 3.035654 | 916.0s\n",
            "step   1880 | loss 3.557223 | best 3.035654 | 920.9s\n",
            "step   1890 | loss 3.589550 | best 3.035654 | 925.8s\n",
            "step   1900 | loss 3.812535 | best 3.035654 | 930.7s\n",
            "step   1910 | loss 3.100225 | best 3.035654 | 935.6s\n",
            "step   1920 | loss 3.895575 | best 3.035654 | 940.5s\n",
            "step   1930 | loss 3.897136 | best 3.035654 | 945.4s\n",
            "step   1940 | loss 3.472364 | best 3.035654 | 950.3s\n",
            "step   1950 | loss 3.789632 | best 3.035654 | 955.2s\n",
            "step   1960 | loss 3.723102 | best 3.035654 | 960.1s\n",
            "step   1970 | loss 3.291262 | best 3.035654 | 965.0s\n",
            "step   1980 | loss 3.615946 | best 2.913211 | 969.9s\n",
            "step   1990 | loss 3.610330 | best 2.913211 | 974.8s\n",
            "step   2000 | loss 3.559333 | best 2.913211 | 979.7s\n",
            "step   2010 | loss 3.242600 | best 2.913211 | 984.6s\n",
            "step   2020 | loss 3.194961 | best 2.913211 | 989.5s\n",
            "step   2030 | loss 3.771070 | best 2.913211 | 994.5s\n",
            "step   2040 | loss 3.606710 | best 2.913211 | 999.4s\n",
            "step   2050 | loss 3.532082 | best 2.913211 | 1004.3s\n",
            "step   2060 | loss 3.387953 | best 2.913211 | 1009.2s\n",
            "step   2070 | loss 3.796828 | best 2.913211 | 1014.1s\n",
            "step   2080 | loss 3.410644 | best 2.913211 | 1019.0s\n",
            "step   2090 | loss 3.489117 | best 2.913211 | 1023.9s\n",
            "step   2100 | loss 3.601146 | best 2.913211 | 1028.8s\n",
            "step   2110 | loss 3.518482 | best 2.913211 | 1033.7s\n",
            "step   2120 | loss 3.347545 | best 2.913211 | 1038.6s\n",
            "step   2130 | loss 3.463961 | best 2.913211 | 1043.5s\n",
            "step   2140 | loss 3.499511 | best 2.730821 | 1048.4s\n",
            "step   2150 | loss 3.635248 | best 2.730821 | 1053.3s\n",
            "step   2160 | loss 3.363110 | best 2.730821 | 1058.2s\n",
            "step   2170 | loss 3.271752 | best 2.730821 | 1063.1s\n",
            "step   2180 | loss 3.182714 | best 2.730821 | 1068.0s\n",
            "step   2190 | loss 3.594069 | best 2.730821 | 1073.0s\n",
            "step   2200 | loss 3.427782 | best 2.730821 | 1077.9s\n",
            "step   2210 | loss 3.182159 | best 2.730821 | 1082.8s\n",
            "step   2220 | loss 3.173774 | best 2.730821 | 1087.7s\n",
            "step   2230 | loss 3.433418 | best 2.730821 | 1092.6s\n",
            "step   2240 | loss 2.793326 | best 2.730821 | 1097.5s\n",
            "step   2250 | loss 3.400657 | best 2.730821 | 1102.4s\n",
            "step   2260 | loss 3.455362 | best 2.730821 | 1107.3s\n",
            "step   2270 | loss 3.049289 | best 2.730821 | 1112.2s\n",
            "step   2280 | loss 3.350118 | best 2.730821 | 1117.1s\n",
            "step   2290 | loss 3.406082 | best 2.730821 | 1122.0s\n",
            "step   2300 | loss 3.009887 | best 2.730821 | 1126.9s\n",
            "step   2310 | loss 3.160490 | best 2.564258 | 1131.8s\n",
            "step   2320 | loss 3.247004 | best 2.564258 | 1136.7s\n",
            "step   2330 | loss 3.180099 | best 2.564258 | 1141.6s\n",
            "step   2340 | loss 2.819624 | best 2.564258 | 1146.5s\n",
            "step   2350 | loss 2.837909 | best 2.564258 | 1151.4s\n",
            "step   2360 | loss 3.355969 | best 2.564258 | 1156.3s\n",
            "step   2370 | loss 3.272974 | best 2.564258 | 1161.2s\n",
            "step   2380 | loss 3.195038 | best 2.564258 | 1166.1s\n",
            "step   2390 | loss 3.016263 | best 2.564258 | 1171.0s\n",
            "step   2400 | loss 3.361660 | best 2.564258 | 1175.9s\n",
            "step   2410 | loss 2.983529 | best 2.564258 | 1180.7s\n",
            "step   2420 | loss 3.033279 | best 2.564258 | 1185.6s\n",
            "step   2430 | loss 3.183084 | best 2.564258 | 1190.5s\n",
            "step   2440 | loss 3.139051 | best 2.564258 | 1195.4s\n",
            "step   2450 | loss 2.879859 | best 2.564258 | 1200.3s\n",
            "step   2460 | loss 3.104761 | best 2.564258 | 1205.3s\n",
            "step   2470 | loss 3.098026 | best 2.451133 | 1210.2s\n",
            "step   2480 | loss 3.159229 | best 2.451133 | 1215.1s\n",
            "step   2490 | loss 2.939664 | best 2.451133 | 1219.9s\n",
            "step   2500 | loss 2.887013 | best 2.451133 | 1224.9s\n",
            "step   2510 | loss 2.747188 | best 2.451133 | 1229.8s\n",
            "step   2520 | loss 3.148036 | best 2.451133 | 1234.6s\n",
            "step   2530 | loss 2.960200 | best 2.451133 | 1239.5s\n",
            "step   2540 | loss 2.747368 | best 2.451133 | 1244.4s\n",
            "step   2550 | loss 2.778216 | best 2.451133 | 1249.4s\n",
            "step   2560 | loss 2.970479 | best 2.451133 | 1254.3s\n",
            "step   2570 | loss 2.425807 | best 2.425807 | 1259.2s\n",
            "step   2580 | loss 2.979206 | best 2.425807 | 1264.1s\n",
            "step   2590 | loss 2.954863 | best 2.425807 | 1269.0s\n",
            "step   2600 | loss 2.624745 | best 2.425807 | 1273.9s\n",
            "step   2610 | loss 2.836594 | best 2.425807 | 1278.8s\n",
            "step   2620 | loss 2.852161 | best 2.425807 | 1283.7s\n",
            "step   2630 | loss 2.629818 | best 2.425807 | 1288.6s\n",
            "step   2640 | loss 2.708505 | best 2.289040 | 1293.5s\n",
            "step   2650 | loss 2.745357 | best 2.289040 | 1298.4s\n",
            "step   2660 | loss 2.767229 | best 2.289040 | 1303.3s\n",
            "step   2670 | loss 2.472470 | best 2.289040 | 1308.3s\n",
            "step   2680 | loss 2.432137 | best 2.289040 | 1313.2s\n",
            "step   2690 | loss 2.921212 | best 2.289040 | 1318.1s\n",
            "step   2700 | loss 2.687387 | best 2.289040 | 1322.9s\n",
            "step   2710 | loss 2.641752 | best 2.289040 | 1327.8s\n",
            "step   2720 | loss 2.486364 | best 2.289040 | 1332.8s\n",
            "step   2730 | loss 2.723378 | best 2.289040 | 1337.7s\n",
            "step   2740 | loss 2.450350 | best 2.250831 | 1342.6s\n",
            "step   2750 | loss 2.516363 | best 2.250831 | 1347.5s\n",
            "step   2760 | loss 2.763117 | best 2.250831 | 1352.4s\n",
            "step   2770 | loss 2.662119 | best 2.244264 | 1357.3s\n",
            "step   2780 | loss 2.362165 | best 2.244264 | 1362.2s\n",
            "step   2790 | loss 2.593796 | best 2.244264 | 1367.1s\n",
            "step   2800 | loss 2.597363 | best 2.059703 | 1372.0s\n",
            "step   2810 | loss 2.637681 | best 2.059703 | 1376.9s\n",
            "step   2820 | loss 2.405512 | best 2.059703 | 1381.8s\n",
            "step   2830 | loss 2.379830 | best 2.059703 | 1386.7s\n",
            "step   2840 | loss 2.282072 | best 2.059703 | 1391.6s\n",
            "step   2850 | loss 2.601728 | best 2.059703 | 1396.5s\n",
            "step   2860 | loss 2.439694 | best 2.059703 | 1401.4s\n",
            "step   2870 | loss 2.171782 | best 2.025064 | 1406.3s\n",
            "step   2880 | loss 2.176260 | best 2.025064 | 1411.2s\n",
            "step   2890 | loss 2.351929 | best 2.025064 | 1416.1s\n",
            "step   2900 | loss 1.958776 | best 1.958776 | 1421.0s\n",
            "step   2910 | loss 2.396885 | best 1.958776 | 1425.9s\n",
            "step   2920 | loss 2.365918 | best 1.958776 | 1430.8s\n",
            "step   2930 | loss 2.067394 | best 1.958776 | 1435.7s\n",
            "step   2940 | loss 2.270463 | best 1.930055 | 1440.6s\n",
            "step   2950 | loss 2.239732 | best 1.930055 | 1445.5s\n",
            "step   2960 | loss 2.116874 | best 1.930055 | 1450.4s\n",
            "step   2970 | loss 2.230410 | best 1.848178 | 1455.3s\n",
            "step   2980 | loss 2.209476 | best 1.848178 | 1460.3s\n",
            "step   2990 | loss 2.164679 | best 1.848178 | 1465.2s\n",
            "step   3000 | loss 1.972047 | best 1.848178 | 1470.1s\n",
            "step   3010 | loss 1.921058 | best 1.848178 | 1475.0s\n",
            "step   3020 | loss 2.250375 | best 1.846419 | 1479.9s\n",
            "step   3030 | loss 2.098356 | best 1.846419 | 1484.8s\n",
            "step   3040 | loss 2.027596 | best 1.788736 | 1489.7s\n",
            "step   3050 | loss 1.930180 | best 1.788736 | 1494.6s\n",
            "step   3060 | loss 2.141701 | best 1.788736 | 1499.5s\n",
            "step   3070 | loss 1.901946 | best 1.745892 | 1504.4s\n",
            "step   3080 | loss 1.982063 | best 1.745892 | 1509.4s\n",
            "step   3090 | loss 2.107011 | best 1.745892 | 1514.3s\n",
            "step   3100 | loss 2.152054 | best 1.643640 | 1519.2s\n",
            "step   3110 | loss 1.868583 | best 1.643640 | 1524.1s\n",
            "step   3120 | loss 1.989630 | best 1.643640 | 1529.0s\n",
            "step   3130 | loss 2.095241 | best 1.619899 | 1533.9s\n",
            "step   3140 | loss 1.993031 | best 1.619899 | 1538.8s\n",
            "step   3150 | loss 1.792308 | best 1.619899 | 1543.7s\n",
            "step   3160 | loss 1.762682 | best 1.619899 | 1548.6s\n",
            "step   3170 | loss 1.683893 | best 1.619899 | 1553.6s\n",
            "step   3180 | loss 1.934741 | best 1.571384 | 1558.5s\n",
            "step   3190 | loss 1.812726 | best 1.571384 | 1563.4s\n",
            "step   3200 | loss 1.580470 | best 1.506085 | 1568.3s\n",
            "step   3210 | loss 1.629544 | best 1.506085 | 1573.2s\n",
            "step   3220 | loss 1.764246 | best 1.506085 | 1578.1s\n",
            "step   3230 | loss 1.481779 | best 1.481779 | 1583.0s\n",
            "step   3240 | loss 1.755458 | best 1.481779 | 1587.9s\n",
            "step   3250 | loss 1.762254 | best 1.481779 | 1592.8s\n",
            "step   3260 | loss 1.538460 | best 1.481779 | 1597.7s\n",
            "step   3270 | loss 1.722170 | best 1.477640 | 1602.7s\n",
            "step   3280 | loss 1.734203 | best 1.477640 | 1607.6s\n",
            "step   3290 | loss 1.645952 | best 1.477640 | 1612.5s\n",
            "step   3300 | loss 1.690730 | best 1.378511 | 1617.4s\n",
            "step   3310 | loss 1.631767 | best 1.378511 | 1622.3s\n",
            "step   3320 | loss 1.592955 | best 1.378511 | 1627.2s\n",
            "step   3330 | loss 1.420742 | best 1.378511 | 1632.1s\n",
            "step   3340 | loss 1.405378 | best 1.351842 | 1637.0s\n",
            "step   3350 | loss 1.639282 | best 1.315594 | 1641.9s\n",
            "step   3360 | loss 1.502521 | best 1.315594 | 1646.8s\n",
            "step   3370 | loss 1.426922 | best 1.253103 | 1651.8s\n",
            "step   3380 | loss 1.350761 | best 1.253103 | 1656.7s\n",
            "step   3390 | loss 1.475214 | best 1.253103 | 1661.6s\n",
            "step   3400 | loss 1.299747 | best 1.241061 | 1666.5s\n",
            "step   3410 | loss 1.387368 | best 1.241061 | 1671.4s\n",
            "step   3420 | loss 1.417181 | best 1.241061 | 1676.3s\n",
            "step   3430 | loss 1.553296 | best 1.181594 | 1681.2s\n",
            "step   3440 | loss 1.350536 | best 1.181594 | 1686.1s\n",
            "step   3450 | loss 1.364846 | best 1.181594 | 1691.0s\n",
            "step   3460 | loss 1.474118 | best 1.144464 | 1695.9s\n",
            "step   3470 | loss 1.432838 | best 1.144464 | 1700.8s\n",
            "step   3480 | loss 1.250782 | best 1.144464 | 1705.7s\n",
            "step   3490 | loss 1.245926 | best 1.144464 | 1710.6s\n",
            "step   3500 | loss 1.114501 | best 1.114501 | 1715.5s\n",
            "step   3510 | loss 1.339231 | best 1.066345 | 1720.4s\n",
            "step   3520 | loss 1.214925 | best 1.066345 | 1725.4s\n",
            "step   3530 | loss 1.123158 | best 1.034296 | 1730.3s\n",
            "step   3540 | loss 1.065780 | best 1.031598 | 1735.2s\n",
            "step   3550 | loss 1.162024 | best 1.031598 | 1740.1s\n",
            "step   3560 | loss 1.024092 | best 1.024092 | 1745.0s\n",
            "step   3570 | loss 1.167188 | best 1.024092 | 1749.9s\n",
            "step   3580 | loss 1.226089 | best 1.024092 | 1754.8s\n",
            "step   3590 | loss 1.021128 | best 1.021128 | 1759.7s\n",
            "step   3600 | loss 1.227236 | best 0.966440 | 1764.6s\n",
            "step   3610 | loss 1.227984 | best 0.966440 | 1769.6s\n",
            "step   3620 | loss 1.041736 | best 0.966440 | 1774.5s\n",
            "step   3630 | loss 1.136687 | best 0.927293 | 1779.4s\n",
            "step   3640 | loss 1.106424 | best 0.927293 | 1784.3s\n",
            "step   3650 | loss 1.078767 | best 0.927293 | 1789.2s\n",
            "step   3660 | loss 0.902361 | best 0.902361 | 1794.1s\n",
            "step   3670 | loss 0.881145 | best 0.854902 | 1799.0s\n",
            "step   3680 | loss 1.086922 | best 0.854902 | 1804.0s\n",
            "step   3690 | loss 0.932312 | best 0.854902 | 1808.9s\n",
            "step   3700 | loss 0.863560 | best 0.801376 | 1813.8s\n",
            "step   3710 | loss 0.812789 | best 0.763857 | 1818.7s\n",
            "step   3720 | loss 0.934461 | best 0.763857 | 1823.6s\n",
            "step   3730 | loss 0.813063 | best 0.763857 | 1828.5s\n",
            "step   3740 | loss 0.836481 | best 0.763857 | 1833.4s\n",
            "step   3750 | loss 0.878364 | best 0.763857 | 1838.3s\n",
            "step   3760 | loss 0.975122 | best 0.763857 | 1843.2s\n",
            "step   3770 | loss 0.813003 | best 0.763857 | 1848.1s\n",
            "step   3780 | loss 0.856265 | best 0.763857 | 1853.0s\n",
            "step   3790 | loss 0.895656 | best 0.697330 | 1858.0s\n",
            "step   3800 | loss 0.903866 | best 0.697330 | 1862.9s\n",
            "step   3810 | loss 0.801555 | best 0.697330 | 1867.8s\n",
            "step   3820 | loss 0.731993 | best 0.697330 | 1872.7s\n",
            "step   3830 | loss 0.643567 | best 0.641948 | 1877.6s\n",
            "step   3840 | loss 0.818292 | best 0.611835 | 1882.5s\n",
            "step   3850 | loss 0.751812 | best 0.611835 | 1887.4s\n",
            "step   3860 | loss 0.662719 | best 0.611835 | 1892.3s\n",
            "step   3870 | loss 0.622505 | best 0.587848 | 1897.2s\n",
            "step   3880 | loss 0.659993 | best 0.577225 | 1902.1s\n",
            "step   3890 | loss 0.571068 | best 0.571068 | 1907.0s\n",
            "step   3900 | loss 0.707828 | best 0.565007 | 1911.9s\n",
            "step   3910 | loss 0.683955 | best 0.565007 | 1916.8s\n",
            "step   3920 | loss 0.631606 | best 0.565007 | 1921.7s\n",
            "step   3930 | loss 0.730673 | best 0.565007 | 1926.6s\n",
            "step   3940 | loss 0.653448 | best 0.565007 | 1931.5s\n",
            "step   3950 | loss 0.622979 | best 0.565007 | 1936.4s\n",
            "step   3960 | loss 0.699120 | best 0.508864 | 1941.4s\n",
            "step   3970 | loss 0.650575 | best 0.508864 | 1946.3s\n",
            "step   3980 | loss 0.638685 | best 0.508864 | 1951.2s\n",
            "step   3990 | loss 0.514074 | best 0.508864 | 1956.1s\n",
            "step   4000 | loss 0.470798 | best 0.454898 | 1961.0s\n",
            "step   4010 | loss 0.590152 | best 0.410986 | 1966.0s\n",
            "step   4020 | loss 0.518838 | best 0.410986 | 1970.9s\n",
            "step   4030 | loss 0.505738 | best 0.410986 | 1975.8s\n",
            "step   4040 | loss 0.388212 | best 0.388212 | 1980.7s\n",
            "step   4050 | loss 0.474262 | best 0.388212 | 1985.6s\n",
            "step   4060 | loss 0.393035 | best 0.388212 | 1990.5s\n",
            "step   4070 | loss 0.395795 | best 0.370781 | 1995.4s\n",
            "step   4080 | loss 0.440285 | best 0.370781 | 2000.3s\n",
            "step   4090 | loss 0.508275 | best 0.370781 | 2005.3s\n",
            "step   4100 | loss 0.441170 | best 0.370781 | 2010.2s\n",
            "step   4110 | loss 0.439873 | best 0.370781 | 2015.1s\n",
            "step   4120 | loss 0.490223 | best 0.370781 | 2020.0s\n",
            "step   4130 | loss 0.505275 | best 0.370781 | 2024.9s\n",
            "step   4140 | loss 0.426170 | best 0.370781 | 2029.8s\n",
            "step   4150 | loss 0.387667 | best 0.370781 | 2034.8s\n",
            "step   4160 | loss 0.327604 | best 0.327604 | 2039.7s\n",
            "step   4170 | loss 0.429731 | best 0.307165 | 2044.6s\n",
            "step   4180 | loss 0.362251 | best 0.307165 | 2049.5s\n",
            "step   4190 | loss 0.335093 | best 0.307165 | 2054.4s\n",
            "step   4200 | loss 0.314843 | best 0.304962 | 2059.3s\n",
            "step   4210 | loss 0.271901 | best 0.271901 | 2064.2s\n",
            "step   4220 | loss 0.250859 | best 0.247394 | 2069.2s\n",
            "step   4230 | loss 0.331652 | best 0.241647 | 2074.1s\n",
            "step   4240 | loss 0.311513 | best 0.241647 | 2079.0s\n",
            "step   4250 | loss 0.297507 | best 0.241647 | 2083.9s\n",
            "step   4260 | loss 0.359586 | best 0.241647 | 2088.8s\n",
            "step   4270 | loss 0.316540 | best 0.241647 | 2093.7s\n",
            "step   4280 | loss 0.271286 | best 0.241647 | 2098.6s\n",
            "step   4290 | loss 0.311775 | best 0.231967 | 2103.5s\n",
            "step   4300 | loss 0.318546 | best 0.231967 | 2108.5s\n",
            "step   4310 | loss 0.278813 | best 0.231967 | 2113.4s\n",
            "step   4320 | loss 0.247959 | best 0.215907 | 2118.3s\n",
            "step   4330 | loss 0.207888 | best 0.204789 | 2123.2s\n",
            "step   4340 | loss 0.266039 | best 0.204789 | 2128.1s\n",
            "step   4350 | loss 0.228566 | best 0.192754 | 2133.0s\n",
            "step   4360 | loss 0.195255 | best 0.192754 | 2137.9s\n",
            "step   4370 | loss 0.186343 | best 0.185717 | 2142.8s\n",
            "step   4380 | loss 0.188879 | best 0.185717 | 2147.8s\n",
            "step   4390 | loss 0.168016 | best 0.164650 | 2152.7s\n",
            "step   4400 | loss 0.176713 | best 0.152605 | 2157.6s\n",
            "step   4410 | loss 0.185320 | best 0.152605 | 2162.5s\n",
            "step   4420 | loss 0.205546 | best 0.152605 | 2167.4s\n",
            "step   4430 | loss 0.191982 | best 0.152605 | 2172.3s\n",
            "step   4440 | loss 0.168646 | best 0.152605 | 2177.2s\n",
            "step   4450 | loss 0.191240 | best 0.145701 | 2182.1s\n",
            "step   4460 | loss 0.185680 | best 0.145701 | 2187.0s\n",
            "step   4470 | loss 0.166551 | best 0.145701 | 2191.9s\n",
            "step   4480 | loss 0.154847 | best 0.145701 | 2196.8s\n",
            "step   4490 | loss 0.129038 | best 0.129038 | 2201.7s\n",
            "step   4500 | loss 0.180436 | best 0.128725 | 2206.6s\n",
            "step   4510 | loss 0.137571 | best 0.128725 | 2211.6s\n",
            "step   4520 | loss 0.141736 | best 0.122512 | 2216.5s\n",
            "step   4530 | loss 0.120525 | best 0.111714 | 2221.4s\n",
            "step   4540 | loss 0.118626 | best 0.104824 | 2226.3s\n",
            "step   4550 | loss 0.120290 | best 0.104824 | 2231.2s\n",
            "step   4560 | loss 0.134991 | best 0.100690 | 2236.1s\n",
            "step   4570 | loss 0.128361 | best 0.100096 | 2241.1s\n",
            "Target loss 0.099999 reached at step 4578, loss=0.094433\n",
            "Saved checkpoint to checkpoints/shakespeare-gpt2-124M.pt\n",
            "\n",
            "=== SAMPLE GENERATION ===\n",
            "ROMEO:\n",
            "Pl had my husband and thy do not his fault\n",
            "And do thee forsware in Exll'd than easy\n",
            "Of her, O,\n",
            "If that way,\n",
            "Since thou touch'd to't, for all his queen,\n",
            "Had oft best\n",
            " brieve a groans,\n",
            "Did not in loving lord, but one sweet king\n",
            "At love my heir, nor any worse than the highness.\n",
            "\n",
            "LEONTES:\n",
            "I am glad well, to see,\n",
            "We do taken,\n",
            "Will complaint to himself\n",
            "The purpose like a criscarly done.\n",
            "\n",
            "PAULINA:\n",
            "I'll tell him life\n",
            "Of such most fine comfort,\n",
            "The provost to look as.\n",
            "\n",
            "LEONTES:\n",
            "You said my life,\n",
            "The gracious lord,\n",
            "More than in a vice: the course of mine.\n",
            "\n",
            "LEONTES:\n",
            "I'll not the fault! why,\n",
            "Not to a king\n",
            "A:\n",
            "\n",
            "Saved sample_output.txt\n"
          ]
        }
      ]
    }
  ]
}